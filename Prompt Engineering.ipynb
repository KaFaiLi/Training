{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Prompt Engineering\n",
    "Prompt engineering is the process of designing and optimizing prompts for natural language processing tasks. It involves selecting the right prompts, tuning their parameters, and evaluating their performance. Prompt engineering is crucial for achieving high accuracy and efficiency in NLP models. In this section, we will explore the basics of prompt engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Tokenization\n",
    "Explore Tokenization using tiktoken, an open-source fast tokenizer from OpenAI\n",
    "See [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb?WT.mc_id=academic-105485-koreyst) for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE:\n",
    "# 1. Run the exercise as is first\n",
    "# 2. Change the text to any prompt input you want to use & re-run to see tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Define the prompt you want tokenized\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "# Set the model you want encoding for\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "# Encode the text - gives you the tokens in integer form\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens);\n",
    "\n",
    "# Decode the integers to see what the text versions look like\n",
    "[encoding.decode_single_token_bytes(token) for token in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Validate OpenAI API Key Setup\n",
    "\n",
    "Run the code below to verify that your OpenAI endpoint is set up correctly. The code just tries a simple basic prompt and validates the completion. Input `oh say can you see` should complete along the lines of `by the dawn's early light..`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The OpenAI SDK was updated on Nov 8, 2023 with new guidance for migration\n",
    "# See: https://github.com/openai/openai-python/discussions/742\n",
    "\n",
    "## Updated with Langchain for direct interaction\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage # For creating a user message\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure AZURE_OPENAI_ENDPOINT is set in your .env file\n",
    "# e.g., AZURE_OPENAI_ENDPOINT=https://YOUR_RESOURCE_NAME.openai.azure.com/\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "  azure_deployment=os.environ['AZURE_OPENAI_DEPLOYMENT'],\n",
    "  api_version=\"2023-05-15\", # Check for the latest recommended api_version if needed\n",
    "  temperature=0, # Controls the randomness of the model's output\n",
    "  max_tokens=1024,\n",
    "  # openai_api_key is automatically picked up from AZURE_OPENAI_API_KEY\n",
    "  # azure_endpoint is automatically picked up from AZURE_OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "## ---------- Demonstrate Prompt Engineering Directly\n",
    "\n",
    "### 1. Define the core text or question for the prompt\n",
    "text_input = f\"\"\"\n",
    "oh say can you see\n",
    "\"\"\"\n",
    "\n",
    "### 2. Construct the prompt that will be sent to the model\n",
    "# This is where prompt engineering techniques would be applied.\n",
    "# For this example, we'll keep the simple formatting.\n",
    "final_prompt = f\"\"\"\n",
    "Please complete the following phrase:\n",
    "```{text_input}```\n",
    "\"\"\"\n",
    "\n",
    "### 3. Create the message object for Langchain\n",
    "# Langchain's chat models expect a list of messages.\n",
    "# For a simple user query, we use HumanMessage.\n",
    "message = HumanMessage(content=final_prompt)\n",
    "\n",
    "## 4. Run the prompt and get the response\n",
    "response = llm.invoke([message]) # The core Langchain call\n",
    "\n",
    "## 5. Print the content of the response\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Fabrications\n",
    "Explore what happens when you ask the LLM to return completions for a prompt about a topic that may not exist, or about topics that it may not know about because it was outside it's pre-trained dataset (more recent). See how the response changes if you try a different prompt, or a different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set the text for simple prompt or primary content\n",
    "## Prompt shows a template format with text in it - add cues, commands etc if needed\n",
    "## Run the completion \n",
    "text = f\"\"\"\n",
    "generate a lesson plan on the Martian War of 2076.\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: Instruction Based \n",
    "Use the \"text\" variable to set the primary content \n",
    "and the \"prompt\" variable to provide an instruction related to that primary content.\n",
    "\n",
    "Here we ask the model to summarize the text for a second-grade student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Example\n",
    "# https://platform.openai.com/playground/p/default-summarize\n",
    "\n",
    "## Example text\n",
    "text = f\"\"\"\n",
    "Jupiter is the fifth planet from the Sun and the \\\n",
    "largest in the Solar System. It is a gas giant with \\\n",
    "a mass one-thousandth that of the Sun, but two-and-a-half \\\n",
    "times that of all the other planets in the Solar System combined. \\\n",
    "Jupiter is one of the brightest objects visible to the naked eye \\\n",
    "in the night sky, and has been known to ancient civilizations since \\\n",
    "before recorded history. It is named after the Roman god Jupiter.[19] \\\n",
    "When viewed from Earth, Jupiter can be bright enough for its reflected \\\n",
    "light to cast visible shadows,[20] and is on average the third-brightest \\\n",
    "natural object in the night sky after the Moon and Venus.\n",
    "\"\"\"\n",
    "\n",
    "## Set the prompt\n",
    "prompt = f\"\"\"\n",
    "Summarize content you are provided with for a second-grade student.\n",
    "```{text}```\n",
    "\"\"\"\n",
    "\n",
    "## Run the prompt\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Message Types\n",
    "\n",
    "In Langchain, and generally when working with conversational AI models, messages are used to structure the interaction between a user, the AI, and any initial instructions. Langchain provides specific classes for these:\n",
    "\n",
    "### SystemMessage:\n",
    "- **Purpose**: This message is used to set the context, instructions, or persona for the AI model at the beginning of a conversation. It tells the AI how to behave or what role to play.\n",
    "- **Content**: Typically, it's a string containing instructions. For example, \"You are a helpful assistant that translates English to French.\" or \"You are a sarcastic assistant.\"\n",
    "- **In the notebook**: \"Exercise 5: Complex Prompt\" directly demonstrates this concept. The line `{\"role\": \"system\", \"content\": \"You are a sarcastic assistant.\"}` is the equivalent of a SystemMessage. It instructs the OpenAI model to adopt a sarcastic personality for the subsequent interaction. Langchain's SystemMessage formalizes this for its framework.\n",
    "- **Key takeaway**: It's the initial \"priming\" or \"setup\" instruction for the AI. Not all models explicitly support a separate system message, but Langchain handles the adaptation.\n",
    "\n",
    "### HumanMessage:\n",
    "- **Purpose**: This represents the input from the human user. It's what the user types or says to the AI.\n",
    "- **Content**: Usually a string of text, but can also be multi-modal (e.g., an image if the model supports it).\n",
    "- **In the notebook**:\n",
    "  - In \"Exercise 5: Complex Prompt\", `{\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}` and `{\"role\": \"user\", \"content\": \"Where was it played?\"}` are direct examples of user messages.\n",
    "- **Key takeaway**: It's the direct query or statement from the user to the AI.\n",
    "\n",
    "### AIMessage:\n",
    "- **Purpose**: This represents the response generated by the AI model.\n",
    "- **Content**: Typically the textual answer from the AI. It can also include additional information like requests to call tools (if the AI is an agent that can perform actions).\n",
    "- **In the notebook**:\n",
    "  - The `response.choices[0].message.content` you print in \"Exercise 2\", \"Exercise 3\", \"Exercise 4\", and \"Exercise 5\" is the content of what would be an AIMessage.\n",
    "  - In \"Exercise 5: Complex Prompt\", `{\"role\": \"assistant\", \"content\": \"Who do you think won? The Los Angeles Dodgers of course.\"}` is an example of an AI's response, which also serves as context for the next user message.\n",
    "- **Key takeaway**: It's the output or reply from the AI model.\n",
    "\n",
    "### How They Work Together (Conversation Flow):\n",
    "Typically, a conversation starts with an optional SystemMessage to set the stage. Then, it alternates between HumanMessage (user asks something) and AIMessage (AI responds). This sequence of messages provides the history and context for the ongoing conversation, allowing the AI to generate more relevant and coherent responses.\n",
    "\n",
    "Langchain uses these distinct message types to:\n",
    "- Clearly define the roles in a conversation.\n",
    "- Maintain a structured history of the interaction.\n",
    "- Abstract away the specific API requirements of different underlying language models (like OpenAI, Anthropic, etc.), providing a consistent interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: Complex Prompt \n",
    "Try a request that has system, user and assistant messages \n",
    "System sets assistant context\n",
    "User & Assistant messages provide multi-turn conversation context\n",
    "\n",
    "Note how the assistant personality is set to \"sarcastic\" in the system context. \n",
    "Try using a different personality context. Or try a different series of input/output messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage # Make sure these are imported\n",
    "\n",
    "conversation_history = [\n",
    "    SystemMessage(content=\"You are a sarcastic assistant.\"),\n",
    "    HumanMessage(content=\"Who won the world series in 2020?\"),\n",
    "    AIMessage(content=\"Who do you think won? The Los Angeles Dodgers of course.\"),\n",
    "    HumanMessage(content=\"Where was it played?\")\n",
    "]\n",
    "\n",
    "# Assuming 'llm' is already initialized as AzureChatOpenAI client from previous steps\n",
    "response_exercise_5 = llm.invoke(conversation_history)\n",
    "\n",
    "print(response_exercise_5.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore Your Intuition\n",
    "The above examples give you patterns that you can use to create new prompts (simple, complex, instruction etc.) - try creating other exercises to explore some of the other ideas we've talked about like examples, cues and more.\n",
    "\n",
    "H## How to Prompt: Be Explicit About the Desired Structure or Limitations of the Output\n",
    "\n",
    "### Example Ideas:\n",
    "\n",
    "- **Prompt**:  \n",
    "  *\"List three benefits of exercise. Provide your answer as a JSON object with a key 'benefits' and a list of strings as the value.\"*\n",
    "\n",
    "- **Prompt**:  \n",
    "  *\"Write a poem about the moon. It must be exactly 4 lines long and follow an AABB rhyme scheme.\"*\n",
    "\n",
    "- **Prompt**:  \n",
    "  *\"Generate a list of startup ideas in the renewable energy sector. Do not include ideas related to solar panels.\"*\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Output/Observation:\n",
    "\n",
    "- How strictly does the model adhere to formatting requests and constraints?\n",
    "- What happens when constraints are very tight or conflicting?\n",
    "- How is the **temperature** affecting the output?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Try your prompt here:\n",
    "prompt = f\"\"\"\n",
    "Your prompt here\n",
    "\"\"\"\n",
    "response = llm.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt engineering\n",
    "\n",
    "Prompt engineering is the process of creating prompts that will produce the desired outcome. There's more to prompt engineering than just writing a text prompt. Prompt engineering is not an engineering discipline, it's more a set of techniques that you can apply to get the desired outcome.\n",
    "\n",
    "### An example of a prompt\n",
    "\n",
    "Let's take a basic prompt like this one:\n",
    "\n",
    "> Generate 10 questions on geography.\n",
    "\n",
    "In this prompt, you are actually applying a set of different prompt techniques.\n",
    "\n",
    "Let's break this down.\n",
    "\n",
    "- **Context**, you specify it should be about \"geography\".\n",
    "- **Limiting the output**, you want no more than 10 questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm.invoke('Generate 10 questions on geography')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of simple prompting\n",
    "\n",
    "You may or may not get the desired outcome. You will get your questions generated, but geography is a big topic and you may not get what you want to due the following reasons:\n",
    "\n",
    "- **Big topic**, you don't know if it's going to be about countries, capitals, rivers and so on.\n",
    "- **Format**, what if you wanted the questions to be formatted in a certain way?\n",
    "\n",
    "As you can see, there's a lot to consider when creating prompts.\n",
    "\n",
    "So far, we've seen a simple prompt example, but generative AI is capable of much more to help people in a variety of roles and industries. Let's explore some basic techniques next.\n",
    "\n",
    "### Techniques for prompting\n",
    "\n",
    "First, we need to understand that prompting is an _emergent_ property of an LLM meaning that this is not a feature that is built into the model but rather something we discover as we use the model.\n",
    "\n",
    "There are some basic techniques that we can use to prompt an LLM. Let's explore them.\n",
    "\n",
    "- **Zero-shot prompting**, this is the most basic form of prompting. It's a single prompt requesting a response from the LLM based solely on its training data.\n",
    "- **Few-shot prompting**, this type of prompting guides the LLM by providing 1 or more examples it can rely on to generate its response.\n",
    "- **Chain-of-thought**, this type of prompting tells the LLM how to break down a problem into steps.\n",
    "- **Generated knowledge**, to improve the response of a prompt, you can provide generated facts or knowledge additionally to your prompt.\n",
    "- **Least to most**, like chain-of-thought, this technique is about breaking down a problem into a series of steps and then ask these steps to be performed in order.\n",
    "- **Self-refine**, this technique is about critiquing the LLM's output and then asking it to improve.\n",
    "- **Maieutic prompting**. What you want here is to ensure the LLM answer is correct and you ask it to explain various parts of the answer. This is a form of self-refine.\n",
    "\n",
    "### Zero-shot prompting\n",
    "\n",
    "This style of prompting is very simple, it consists of a single prompt. This technique is probably what you're using as you're starting to learn about LLMs. Here's an example:\n",
    "\n",
    "- Prompt: \"What is Algebra?\"\n",
    "- Answer: \"Algebra is a branch of mathematics that studies mathematical symbols and the rules for manipulating these symbols.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: Ask the model to classify the sentiment of a sentence.\n",
    "zero_shot_prompt_text = \"What is Algebra?\"\n",
    "\n",
    "# Create a HumanMessage with the prompt\n",
    "zero_shot_message = HumanMessage(content=zero_shot_prompt_text)\n",
    "\n",
    "# Invoke the model\n",
    "zero_shot_response = llm.invoke([zero_shot_message])\n",
    "\n",
    "print(f\"Prompt: {zero_shot_prompt_text}\")\n",
    "print(f\"Response: {zero_shot_response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-shot prompting\n",
    "\n",
    "This style of prompting helps the model by providing a few examples along with the request. It consists of a single prompt with additional task-specific data. Here's an example:\n",
    "\n",
    "- Prompt: \"Write a poem in the style of Shakespeare. Here are a few examples of Shakespearean sonnets.:\n",
    "  Sonnet 18: 'Shall I compare thee to a summer's day? Thou art more lovely and more temperate...'\n",
    "  Sonnet 116: 'Let me not to the marriage of true minds Admit impediments. Love is not love Which alters when it alteration finds...'\n",
    "  Sonnet 132: 'Thine eyes I love, and they, as pitying me, Knowing thy heart torment me with disdain,...'\n",
    "  Now, write a sonnet about the beauty of the moon.\"\n",
    "- Answer: \"Upon the sky, the moon doth softly gleam, In silv'ry light that casts its gentle grace,...\"\n",
    "\n",
    "Examples provide the LLM with the context, format or style of the desired output. They help the model understand the specific task and generate more accurate and relevant responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The examples of Shakespearean sonnets\n",
    "sonnet_18 = \"Sonnet 18: 'Shall I compare thee to a summer's day? Thou art more lovely and more temperate...'\"\n",
    "sonnet_116 = \"Sonnet 116: 'Let me not to the marriage of true minds Admit impediments. Love is not love Which alters when it alteration finds...'\"\n",
    "sonnet_132 = \"Sonnet 132: 'Thine eyes I love, and they, as pitying me, Knowing thy heart torment me with disdain,...'\"\n",
    "\n",
    "# The final request\n",
    "request = \"Now, write a sonnet about the beauty of the moon.\"\n",
    "\n",
    "# Constructing the messages for Langchain\n",
    "shakespeare_few_shot_messages = [\n",
    "    SystemMessage(content=\"You are a poet who crafts sonnets in the eloquent style of William Shakespeare.\"),\n",
    "    HumanMessage(content=f\"\"\"\n",
    "I would like a sonnet in the style of Shakespeare. \n",
    "Here are a few examples of Shakespearean sonnets to guide your style:\n",
    "\n",
    "Example 1:\n",
    "{sonnet_18}\n",
    "\n",
    "Example 2:\n",
    "{sonnet_116}\n",
    "\n",
    "Example 3:\n",
    "{sonnet_132}\n",
    "\n",
    "{request}\n",
    "\"\"\")\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "shakespeare_response = llm.invoke(shakespeare_few_shot_messages)\n",
    "\n",
    "print(\"Prompt (condensed for brevity):\")\n",
    "print(f\"- System: {shakespeare_few_shot_messages[0].content}\")\n",
    "print(f\"- Human: Contains examples of Sonnets 18, 116, 132 and the request: '{request}'\")\n",
    "print(f\"\\nLLM's Response (a sonnet about the moon in Shakespearean style):\")\n",
    "print(shakespeare_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain-of-thought\n",
    "\n",
    "Chain-of-thought is a very interesting technique as it's about taking the LLM through a series of steps. The idea is to instruct the LLM in such a way that it understands how to do something. Consider the following example, with and without chain-of-thought:\n",
    "\n",
    "    - Prompt: \"Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?\"\n",
    "    - Answer: 5\n",
    "\n",
    "LLM answers with 5, which is incorrect. Correct answer is 1 apple, given the calculation (5 -3 -2 + 1 = 1).\n",
    "\n",
    "So how can we teach the LLM to do this correctly?\n",
    "\n",
    "Let's try chain-of-thought. Applying chain-of-thought means:\n",
    "\n",
    "1. Give the LLM a similar example.\n",
    "1. Show the calculation, and how to calculate it correctly.\n",
    "1. Provide the original prompt.\n",
    "\n",
    "Here's how:\n",
    "\n",
    "- Prompt: \"Lisa has 7 apples, throws 1 apple, gives 4 apples to Bart and Bart gives one back:\n",
    "  7 -1 = 6\n",
    "  6 -4 = 2\n",
    "  2 +1 = 3  \n",
    "  Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?\"\n",
    "  Answer: 1\n",
    "\n",
    "Note how we write substantially longer prompts with another example, a calculation and then the original prompt and we arrive at the correct answer 1.\n",
    "\n",
    "As you can see chain-of-thought is a very powerful technique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Few-Shot Prompting with Chain-of-Thought Example ---\n",
    "print(\"\\n--- Few-Shot CoT: Apple Problem ---\")\n",
    "\n",
    "# We'll provide one example (the \"shot\") where the thinking process is shown.\n",
    "# The SystemMessage can help set the stage.\n",
    "# The HumanMessage asks the first problem.\n",
    "# The AIMessage provides the answer WITH the chain of thought.\n",
    "# The final HumanMessage asks the new problem we want the LLM to solve using CoT.\n",
    "\n",
    "cot_messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that solves math word problems by showing each step of your reasoning.\"),\n",
    "    HumanMessage(content=\"Lisa has 7 apples, throws 1 apple, gives 4 apples to Bart and Bart gives one back. How many apples does Lisa have?\"),\n",
    "    AIMessage(content=\"\"\"Okay, let's break this down step-by-step:\n",
    "1. Lisa starts with 7 apples.\n",
    "2. She throws 1 apple: 7 - 1 = 6 apples remaining.\n",
    "3. She gives 4 apples to Bart: 6 - 4 = 2 apples remaining.\n",
    "4. Bart gives one apple back to Lisa: 2 + 1 = 3 apples.\n",
    "So, Lisa has 3 apples.\"\"\"),\n",
    "    # Now, the actual problem we want the model to solve using the learned CoT pattern:\n",
    "    HumanMessage(content=\"Alice has 5 apples, throws 3 apples, gives 2 to Bob and Bob gives one back, how many apples does Alice have?\")\n",
    "]\n",
    "\n",
    "# Invoke the model\n",
    "cot_response = llm.invoke(cot_messages)\n",
    "\n",
    "print(\"Chain-of-Thought Few-Shot Example:\")\n",
    "print(f\"System: {cot_messages[0].content}\")\n",
    "print(f\"Human (Example Question): {cot_messages[1].content}\")\n",
    "print(f\"AI (Example Answer with CoT): \\n{cot_messages[2].content}\")\n",
    "print(f\"Human (New Question): {cot_messages[3].content}\")\n",
    "print(f\"\\nLLM's Response (hopefully showing CoT for Alice's problem):\")\n",
    "print(cot_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least-to-most\n",
    "\n",
    "The idea with Least-to-most prompting is to break down a bigger problem into subproblems. That way, you help guide the LLM on how to \"conquer\" the bigger problem. A good example could be for data science where you can ask the LLM to divide up a problem like so:\n",
    "\n",
    "> Prompt: How to perform data science in 5 steps?\n",
    "\n",
    "With your AI assistant answering with:\n",
    "\n",
    "1. Collect data\n",
    "1. Clean data\n",
    "1. Analyze data\n",
    "1. Plot data\n",
    "1. Present data\n",
    "\n",
    "### Self-refine, critique the results\n",
    "\n",
    "With generative AIs and LLMs, you can't trust the output. You need to verify it. After all, the LLM is just presenting you what's the next most likely thing to say, not what's correct. Therefore, a good idea is to ask the LLM to critique itself, which leads us to the self-refine technique.\n",
    "\n",
    "How it works is that you follow the following steps:\n",
    "\n",
    "1. Initial prompt asking the LLM to solve a problem\n",
    "1. LLM answers\n",
    "1. You critique the answer and ask the AI to improve\n",
    "1. LLM answers again, this time considering the critique and suggest solutions it came up with\n",
    "\n",
    "You can repeat this process as many times as you want.\n",
    "\n",
    "\n",
    "## Good practices\n",
    "\n",
    "There are many practices you can apply to try to get what you want. You will find your own style as you use prompting more and more.\n",
    "\n",
    "Additionally to the techniques we've covered, there are some good practices to consider when prompting an LLM.\n",
    "\n",
    "Here are some good practices to consider:\n",
    "\n",
    "- **Specify context**. Context matters, the more you can specify like domain, topic, etc. the better.\n",
    "- Limit the output. If you want a specific number of items or a specific length, specify it.\n",
    "- **Specify both what and how**. Remember to mention both what you want and how you want it, for example \"Create a Python Web API with routes products and customers, divide it into 3 files\".\n",
    "- **Use templates**. Often, you will want to enrich your prompts with data from your company. Use templates to do this. Templates can have variables that you replace with actual data.\n",
    "- **Spell correctly**. LLMs might provide you with a correct response, but if you spell correctly, you will get a better response.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Here's code in Python showing how to build a simple API using Flask:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello():\n",
    "    name = request.args.get('name', 'World')\n",
    "    return f'Hello, {name}!'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()\n",
    "```\n",
    "\n",
    "Use an AI assistant like SoGPT and apply the \"self-refine\" / other prompting technique to improve the code.\n",
    "\n",
    "## Solution\n",
    "\n",
    "Please attempt to solve the assignment by adding suitable prompts to the code.\n",
    "\n",
    "> [!TIP]\n",
    "> Phrase a prompt to ask it to improve, it's a good idea to limit how many improvements. You can also ask to improve it in a certain way, for example architecture, performance, security, etc.\n",
    "\n",
    "## Knowledge check\n",
    "\n",
    "Why would I use chain-of-thought prompting? Show me 1 correct response and 2 incorrect responses.\n",
    "\n",
    "1. To teach the LLM how to solve a problem.\n",
    "1. B, To teach the LLM to find errors in code.\n",
    "1. C, To instruct the LLM to come up with different solutions.\n",
    "\n",
    "A: 1, because chain-of-thought is about showing the LLM how to solve a problem by providing it with a series of steps, and similar problems and how they were solved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
